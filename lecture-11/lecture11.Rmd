---
title: "Lecture 11: Spatial Data and Web Services"
subtitle: "Intro to Data Science for Public Policy, Spring 2016"
author: "by Jeff Chen & Dan Hammer, Georgetown University McCourt School of Public Policy"
output: 
  html_document: 
    theme: journal
    toc: yes
---



The objective of this section is to introduce spatial analysis and web service APIs in `R`.  The auxiliary objectives include learning basic web mapping through [Carto](https://www.carto.com) and practicing some classification techniques.  We will focus on two applications -- farmers markets and wind turbines in the United States.  

### Categorizing farmers markets

Start with a very compact, tangible question.  Does state-level policy has an appreciable impact on the composition of farmers' markets in the Southwest?  To answer this question, we pull [a data set from data.gov](https://explore.data.gov/d/wfna-38ey) on 7,863 farmers' markets in the United States. Rather than trusting that the federal government will continue to maintain the valuable public service, data.gov, I have saved the file as [`farmers-mkts.csv`](https://github.com/GeorgetownMcCourt/data-science/blob/master/lecture-11/farmers-mkts.csv) in this directory.  Download the CSV, open it with `read.csv()` and examine a few columns.

```{r comment=NA, cache=FALSE, message=FALSE}
data <- read.csv("farmers-mkts.csv", header=TRUE)
head(data[c("State", "x", "y", "Meat")])
```

The only difference between this data set and the data tables we've dealt with in previous sections is that each record is tied to a specific place on earth.  The column `x` represents the farmers' market's longitude, and `y` represents the market's latitude.  Each record can therefore be represented as a point on a map.  There are two basic data models in geographic information systems (GIS):

1. **Vector**.  A representation of the world using points, lines, and polygons. Vector models are useful for storing data that has discrete boundaries, such as country borders, land parcels, and streets.
2. **Raster**. A representation of the world as a surface divided into a regular grid of cells. Raster models are useful for storing data that varies continuously across space, like an aerial photograph, a satellite image, a surface of chemical concentrations, or an elevation surface.

These are stored in multiple data formats, which have fragmented over time based on GIS software providers.  For example, for vector data, shapefiles (`.shp`) were invented by ESRI, whereas keyhold markup language (`.kml`) files were invented by Keyhole (which was acquired by Google and turned into Google Maps).  Raster formats may be even more diverse.  However, `R` has extension libraries to deal with most of these formats now.  For vector data, we will rely on open formats, and mainly [GeoJSON](http://geojson.org/), which is an extension of the basic [JSON](http://www.json.org/) data format to handle geographic information.  It is the direction GIS is headed.

Ignore all that for a moment.  Use the [`maps`](https://cran.r-project.org/web/packages/maps/index.html) package to use map data that is already in `R` formats.  Note that there is another package, [`mapproj`](https://cran.r-project.org/web/packages/mapproj/index.html), that deals with reprojection.  There are many standards to convert a 3D globe into a 2D map, called projections.  We ignore this, too, for the time being, and rely on the sensible projection built into the [`maps`](https://cran.r-project.org/web/packages/maps/index.html) package.  Create a map of all farmers' markets in the U.S. with boundary lines (noting that the `"state"` map is built into the `maps` library).

```{r comment=NA, cache=FALSE, message=FALSE}
library(maps)
map("state", interior=FALSE)
map("state", boundary=FALSE, col="gray", add=TRUE)
points(data$x, data$y, cex=0.2, col="blue")
```

Now consider the farmers' markets in Colorado, Utah, New Mexico, and Arizona.  How many farmers' markets are in these four states?  Create a subset of the data with markets in just these four states, and call it `state.data`.  Note that one market is mislabeled, and it is actually in Pennsylvania.  *Knock this out by limiting the farmers' markets based on longitude.*

```{r comment=NA, cache=FALSE, message=FALSE}
state.data <- data[data$State %in% c("Colorado", "Utah", "New Mexico", "Arizona"),]
state.data <- state.data[state.data$x < -90,]
X <- state.data[8:ncol(state.data)]
X <- apply(X, 2, function(col) { ifelse(col=="Y", 1, 0)})
colnames(X)
```

Each column of `state.data` contains information on a different attribute of the market.  The last 24 columns are binary variables with entries `"Y"` or `"N"`, indicating whether the market sells cheese, for example, or accepts credit cards.  Is it possible to predict the location of farmers' markets, based purely on these features?  If so, then there may be something about state policy that has an observable, immediate impact on the composition of the markets.  Clean up the features, assigning a numerical indicator to the `"Y"` response.  Put these binary variables into a data frame called `X`.

```{r comment=NA, cache=FALSE, message=FALSE}
```

Create a distance matrix (`dist.mat`) between the markets in `X` to measure how similar each market is to every other.  Distance does not refer to geographical distance.  Rather, here, it is measure of how similar one market from every other.  Note that all variables in the feature matrix `X` are binary.  Use the `"binary"` method to calculate distance in order to get a meaningful metric of similarity.

```{r comment=NA, cache=FALSE, message=FALSE}
dist.mat <- dist(X, method = "binary")
```

The `dist.mat` object is the basis for the hierarchical clustering algorithm in `R`, which sorts the markets to minimize distance between all elements.  Use the `hclust` function to create four clusters, one for each state.  Build and plot the tree.

```{r comment=NA, cache=FALSE, message=FALSE}
hclust.res <- hclust(dist.mat)
cl <- cutree(hclust.res, k=4)
plot(cut(as.dendrogram(hclust.res), h=0)$upper, leaflab="none")
```

Create a zoomed map, where the farmers' markets in the four states are colored by the label generated in the clustering algorithm.  The color is based on similarity among the bundle of goods sold at each market.  Is this related in some way to geographical clustering?  It is related in a weird way?  Something worth studying further?

```{r comment=NA, cache=TRUE, message=FALSE}

assignColor <- function(cl.idx) {
	col.codes <- c("#FF8000", "#0080FF", "#FFBF00", "#FF4000")
	return(col.codes[cl.idx])
}

map("state", interior=FALSE, xlim=c(-117, -101), ylim=c(28,43))
map("state", boundary=FALSE, col="gray", add=TRUE, xlim=c(-117, -101), ylim=c(28,43))
points(state.data$x, state.data$y, cex=1, pch=20, col=assignColor(cl))
```

It seems clear from these figures that farmers' markets in New Mexico are distinctive from those in neighboring states, somehow.  We can force the analysis into a traditional-ish regression discontinuity test.  First, calculate the distance of each market to the New Mexico border between Arizona and Colorado.  I have plugged in the three points to define `segment`, the upside-down and backwards L-shaped border.  Use this within the function to calculate distance to the New Mexico border.

```{r}
segment <<- cbind(
	c(-109.047546, -109.047546, -103.002319),
	c(31.33487100, 36.99816600, 36.99816600)
)
```

```{r comment=NA, cache=TRUE, message=FALSE}
library(maptools)

.segDistance <- function(coord) {
	segment <<- cbind(
		c(-109.047546, -109.047546, -103.002319),
		c(31.33487100, 36.99816600, 36.99816600)
	)
	near.obj <- nearestPointOnSegment(segment, coord)
	return(as.numeric(near.obj[["distance"]]))
}
```

Note the use of `<<-`. The expressly local function `.segDistance` will return the distance between the supplied coordinate to the global line segment.  Apply this function to all coordinates.  The resulting object `dist` represents distance to the New Mexico border; and to indicate the side of the border, scale the distance for each market /within/ New Mexico by $-1$.  A distance of zero indicates the border itself.  This is beginning to look more and more like the regression discontinuity design, with the discontinuity at zero distance.

```{r comment=NA, cache=TRUE, message=FALSE}
coords <- state.data[ , c("x", "y")]
dist <- apply(coords, 1, FUN=.segDistance)
dist <- dist * ifelse(state.data["State"] == "New Mexico", -1, 1)

```

Now, plot the predicted cluster with respect to distance from border.
Figure indicates a clear discontinuity at the
border.  Note, however, that the regression discontinuity analysis
that we learn is generally for functions, not correspondences.

```{r comment=NA, cache=TRUE, message=FALSE}
set.cl <- cl <= 4
plot(dist[set.cl], cl[set.cl], 
	pch = 20, col="blue", 
	xlab="Distance to New Mexico border",
	ylab="Cluster category",
	yaxt="n"
)

abline(v=0, lty=3, col="red")
axis(2, at = 1:4)
```

The plot indicates some discontinuity (technically the correspondence is not hemicontinuous).  All the figures combined offer reasonably strong evidence that the New Mexico border  alters the composition of markets.  Why?  What causes that?  More digging is required.


### Spatial queries via web services

The Audobon Society estimates that around 250,000 birds are killed each year by wind turbines.  Policy to mitigate this problem will be inherently spatial -- optimizing turbine placement by factoring in bird routes.  We will work with a dataset of U.S. wind turbines and another of California bird routes to identify the number of turbines that are *close* to bird routes.  We will do this using `R` and Carto, an online spatial database.

***

##### Interacting with Carto (optional)

Only follow the following steps if you want to explore Carto after class.  You will be able to query my account for the rest of this lecture, without one of your own.  That said, Carto is a powerful abstraction over a cloud SQL database with the PostGIS extension.  It's a neat way to view, convert, and store spatial data.  It's not cheap for production apps, but it's easily accessible for quick map visualizations.

1. Go to [Carto](https://www.carto.com) and create a free account.  Sign up with your **.edu** address, since there are features available to students for free.  You can find these deals through the [GitHub student developer pack](https://education.github.com/pack).
2. Connect the two datasets, [wind turbines](https://dangeorge.carto.com/tables/turbines/public) and [bird routes](https://dangeorge.carto.com/tables/bird_routes/public).  You can click the button `CREATE MAP` to connect the database from my account to your account.
3. Open the data table view for the turbines layer.  Preview the data to make sure it looks right.  
4. Write a query to count the number of turbines in California.
```sql
SELECT COUNT(*) 
FROM turbines
WHERE state = 'CA'
```
5. Write an SQL query to view just the turbines with a blade length greater than 10 meters in California.  Preview the result.
```sql
SELECT * 
FROM turbines
WHERE state = 'CA' AND blade_l > 10
```
6. Write a query to count the turbines that are within 5 miles of a California bird route.  Refer to the documentation for [`ST_DWithin`](https://postgis.net/docs/ST_DWithin.html).  Note that we use the variable `the_geom_webmercator`.  This is an automatic variable included in *all* Carto tables to run distance or area queries.  It is the standard geometry, but with a [Web Mercator](https://en.wikipedia.org/wiki/Web_Mercator) projection.  *If you're interested, use the variable `the_geom`, see what happens, and try to explain it.*
```sql
SELECT 
	COUNT(turbines.*)
FROM 
	bird_routes, turbines
WHERE
	ST_DWithin(
		bird_routes.the_geom_webmercator,
		turbines.the_geom_webmercator,
		5 * 1609
	)
```

***

#### Spatial web services

We can query Carto databases via web service APIs outside of the Carto environment.  Your public tables are available for SQL queries at the base URL:

```
https://<your_user_name>.carto.com/api/v2/sql
```

In my case, the base URL is:

```
https://dangeorge.carto.com/api/v2/sql
```

Anyone can hit this URL with a parameter `q` followed by the SQL code.  (There is no authentication required.) Copy and paste the SQL query from Item #4 above to count the number of turbines in California after the base URL and the parameter submission `?q=` as below:

```
https://dangeorge.carto.com/api/v2/sql?q=SELECT COUNT(*) FROM turbines WHERE state = 'CA'
```

```json
{
	rows: [
		{
			count: 14524
		}
	],
	time: 0.031,
	fields: {
		count: {
			type: "number"
		}
	},
	total_rows: 1
}
```

This is a powerful tool to run massive, spatial queries on someone else's servers, and then just get back the processed result for use in `R`.  Within `R`, the only requirement is to write, submit, and process the results of a URL. For this, we rely heavily on the `paste`, `URLencode`, and `fromJSON` functions.  You will need to install the `jsonlite` and `curl` libraries.  Write a short `R` script to return just the count of turbines in California.

```{r comment=NA, cache=TRUE, message=FALSE}
library(curl)
library(jsonlite)

url <- "https://dangeorge.carto.com/api/v2/sql?q=SELECT COUNT(*) FROM turbines WHERE state = 'CA'"
res <- fromJSON(URLencode(url))
print(res$rows$count)
```

We can make this more complicated but more useful.  Write a function that accepts an integer number of miles and returns the number of turbines within that many miles of a California bird route.


```{r comment=NA, cache=TRUE, message=FALSE}
turbines <- function(miles) {

	base <- "https://dangeorge.carto.com/api/v2/sql?q="
	pre <- "SELECT 
			COUNT(turbines.*)
			FROM 
			bird_routes, turbines
			WHERE
			ST_DWithin(
			    bird_routes.the_geom_webmercator,
			    turbines.the_geom_webmercator,"

	meters <- miles * 1609
	url <- paste0(base, pre, meters, ")")
	res <- fromJSON(URLencode(url))
	return(res$rows$count)
}

turbines(5)
turbines(4)
turbines(3)
turbines(2)
```

Test the function by running it with a few numbers.  

### Web service APIs, practice

Spatial data and web service APIs are, together, a very powerful combination for quick prototypes and motivating research questions.  Let's practice that hack.  Quickly hitting an API to create an `R` visualization or analysis.

We will rely on the [ProPublica Nonprofit Explorer API (v1)](https://projects.propublica.org/nonprofits/api-v1).  Create a histogram of the revenue from the 100 top earning nonprofits in the database.  Note that only 25 results are returned per page.  Note, also, that there are multiple entries for each nonprofit.  Just get the average revenue for each of the filings in the first 20 pages of results.  This should be more than sufficient to collect the 100 top earners.

```{r comment=NA, cache=TRUE, message=FALSE}
getPage <- function(page) {
	base <- "https://projects.propublica.org/nonprofits/api/v1/search.json"
	params <- paste0("order=revenue&sort_order=desc&page=", page)
	url <- paste0(base, "?", params)
	res <- fromJSON(URLencode(url), flatten=TRUE)
	sub <- data.frame(
		revenue=res$filings$totrevenue,
		name=res$filings$organization.name
	)
	return(sub)
}

res <- list()

for (p in 0:10) {
	res[[p+1]] <- getPage(p)
}

total <- rbind.pages(res)
collapsed <- aggregate(total, by=list(total$name), FUN=mean, na.rm=TRUE)
hist(collapsed$revenue)

```










